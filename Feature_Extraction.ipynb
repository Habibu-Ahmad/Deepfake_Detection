{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. INSTALL DEPENDENCIES**"
      ],
      "metadata": {
        "id": "K2oZWlyju6ln"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PucNaxpvuqG-",
        "outputId": "87e827fb-9b13-498d-b320-a17e13173d63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyWavelets\n",
            "  Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.11/dist-packages (9.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.4.2)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (11.2.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.3.30)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyWavelets\n",
            "Successfully installed PyWavelets-1.8.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install PyWavelets scikit-image pandas tenacity numpy scipy tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imports**"
      ],
      "metadata": {
        "id": "h4ETO9HlvKbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Import all necessary libraries\"\"\"\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shutil\n",
        "import logging\n",
        "import time\n",
        "from skimage import io as skio\n",
        "from skimage import color, measure, feature\n",
        "from skimage.util import random_noise, img_as_float\n",
        "from scipy import ndimage as ndi\n",
        "from skimage.restoration import estimate_sigma\n",
        "from tqdm import tqdm\n",
        "from multiprocessing import Pool, cpu_count\n",
        "import glob\n",
        "from google.colab import drive\n",
        "from IPython.display import Javascript, display"
      ],
      "metadata": {
        "id": "HvgZGWJ0vOMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. MOUNT GOOGLE DRIVE**"
      ],
      "metadata": {
        "id": "NGZKjVEIxgCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Mount Google Drive to access image folders\"\"\"\n",
        "if not os.path.exists('/content/drive'):\n",
        "  drive.mount('/content/drive')\n",
        "else:\n",
        "  print(\"Drive already mounted.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXlEWBQjxfem",
        "outputId": "614507e4-7e48-4cbc-9332-df3581ea2d8e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configurations**"
      ],
      "metadata": {
        "id": "Ou7UpYYwvcHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List all folders containing your images (update these paths)\n",
        "IMAGE_FOLDERS = [\n",
        "    '/content/drive/MyDrive/Deepfake_Images/real_train',\n",
        "    '/content/drive/MyDrive/Deepfake_Images/real_test',\n",
        "    '/content/drive/MyDrive/Deepfake_Images/fake_train',\n",
        "    '/content/drive/MyDrive/Deepfake_Images/fake_test',\n",
        "\n",
        "]\n",
        "\n",
        "# Output directory for results\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/Deepfake_Dataset\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# File paths for tracking progress\n",
        "PROCESSED_FILES_LOG = os.path.join(OUTPUT_DIR, \"processed_files.log\")\n",
        "BACKUP_DIR = os.path.join(OUTPUT_DIR, \"backups\")\n",
        "os.makedirs(BACKUP_DIR, exist_ok=True)\n",
        "\n",
        "# Final output files\n",
        "COMBINED_OUTPUT = os.path.join(OUTPUT_DIR, \"all_features_combined.csv\")"
      ],
      "metadata": {
        "id": "QDhj1qf9ve7q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. ANTI-DISCONNECT**"
      ],
      "metadata": {
        "id": "BaEy_l69yi01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Prevent Colab from disconnecting during long runs\"\"\"\n",
        "display(Javascript('''\n",
        "function KeepAlive(){\n",
        "    console.log(\"Session active\");\n",
        "    google.colab.kernel.proxyPort(5000, {})\n",
        "}\n",
        "setInterval(KeepAlive, 60*1000);\n",
        "'''))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "gS_R-Q3WyZsQ",
        "outputId": "76185915-06a4-49d7-9481-3986a0a5f454"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "function KeepAlive(){\n",
              "    console.log(\"Session active\");\n",
              "    google.colab.kernel.proxyPort(5000, {})\n",
              "}\n",
              "setInterval(KeepAlive, 60*1000);\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. FEATURE EXTRACTION**"
      ],
      "metadata": {
        "id": "jKsJKd-Qy4b8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(img_path):\n",
        "    \"\"\"\n",
        "    Extracts the following features from an image:\n",
        "    - entropy\n",
        "    - wrapped phase range\n",
        "    - noise estimate\n",
        "    - blur measure\n",
        "    - keypoint count\n",
        "    - blob count\n",
        "    - label (real/fake)\n",
        "\n",
        "    Returns: Dictionary with all 7 features or None if failed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read image\n",
        "        img = skio.imread(img_path)\n",
        "        if img is None:\n",
        "            logging.warning(f\"Could not read image: {img_path}\")\n",
        "            return None\n",
        "\n",
        "        # Convert to grayscale for some features\n",
        "        gray_img = color.rgb2gray(img)\n",
        "\n",
        "        # 1. Calculate entropy\n",
        "        entropy = measure.shannon_entropy(img)\n",
        "\n",
        "        # 2. Calculate wrapped phase range\n",
        "        image_wrapped = np.angle(np.exp(1j * img))\n",
        "        wrapped = np.max(image_wrapped) - np.min(image_wrapped)\n",
        "\n",
        "        # 3. Estimate noise level\n",
        "        astro = img_as_float(img)[30:180, 150:300]  # Sample a region\n",
        "        noisy = random_noise(astro, var=0.08**2)\n",
        "        noise = np.mean(estimate_sigma(noisy, channel_axis=-1))\n",
        "\n",
        "        # 4. Measure blur (average of multiple filter sizes)\n",
        "        blur = np.mean([ndi.uniform_filter(img, size=k) for k in range(2, 32, 2)])\n",
        "\n",
        "        # 5. Count keypoints using CENSURE detector\n",
        "        detector = feature.CENSURE()\n",
        "        detector.detect(gray_img)\n",
        "        keypoints = len(detector.keypoints)\n",
        "\n",
        "        # 6. Count blobs using Difference of Gaussian\n",
        "        blobs = len(feature.blob_dog(gray_img, max_sigma=1, threshold=0.1))\n",
        "\n",
        "        # 7. Determine label from path\n",
        "        label = 'real' if 'real' in img_path.lower() else 'fake'\n",
        "\n",
        "        return {\n",
        "            'file_path': img_path,\n",
        "            'entropy': entropy,\n",
        "            'wrapped': wrapped,\n",
        "            'noise': noise,\n",
        "            'blur': blur,\n",
        "            'keypoints': keypoints,\n",
        "            'blobs': blobs,\n",
        "            'label': label\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing {img_path}: {str(e)}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "mBbr1IaVy5go"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** HELPER FUNCTIONS**"
      ],
      "metadata": {
        "id": "FXwHQwPMz9XZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"Supporting functions for processing and tracking\"\"\"\n",
        "\n",
        "def get_folder_features_path(folder_path):\n",
        "    \"\"\"Generate output CSV path for a specific folder\"\"\"\n",
        "    folder_name = os.path.basename(folder_path)\n",
        "    return os.path.join(OUTPUT_DIR, f\"{folder_name}_features.csv\")\n",
        "\n",
        "def load_processed_files():\n",
        "    \"\"\"Load set of already processed files from log\"\"\"\n",
        "    processed = set()\n",
        "    if os.path.exists(PROCESSED_FILES_LOG):\n",
        "        with open(PROCESSED_FILES_LOG, 'r') as f:\n",
        "            processed = set(line.strip() for line in f)\n",
        "    return processed\n",
        "\n",
        "def save_folder_results(folder_path, results):\n",
        "    \"\"\"Save results without file_path column\"\"\"\n",
        "    output_path = get_folder_features_path(folder_path)\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(output_path, index=False)\n",
        "\n",
        "def update_processed_log(results):\n",
        "    \"\"\"Update the log of processed files\"\"\"\n",
        "    with open(PROCESSED_FILES_LOG, 'a') as f:\n",
        "        for r in results:\n",
        "            f.write(f\"{r['file_path']}\\n\")\n",
        "\n",
        "def combine_all_results():\n",
        "    \"\"\"Combine all individual folder CSVs into one master file\"\"\"\n",
        "    all_dfs = []\n",
        "    for folder in IMAGE_FOLDERS:\n",
        "        csv_path = get_folder_features_path(folder)\n",
        "        if os.path.exists(csv_path):\n",
        "            df = pd.read_csv(csv_path)\n",
        "            all_dfs.append(df)\n",
        "\n",
        "    if all_dfs:\n",
        "        combined_df = pd.concat(all_dfs, ignore_index=True)\n",
        "        combined_df.to_csv(COMBINED_OUTPUT, index=False)\n",
        "        logging.info(f\"Combined results saved to {COMBINED_OUTPUT}\")\n",
        "    else:\n",
        "        logging.warning(\"No individual folder results found to combine\")"
      ],
      "metadata": {
        "id": "SxkQWSkDz7BB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PARALLEL PROCESSING**"
      ],
      "metadata": {
        "id": "xIv4XOZN04Ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_folder(folder_path):\n",
        "    \"\"\"Process all images in a single folder\"\"\"\n",
        "    if not os.path.exists(folder_path):\n",
        "        logging.warning(f\"Folder not found: {folder_path}\")\n",
        "        return []\n",
        "\n",
        "    # Get all image paths in this folder\n",
        "    extensions = ('*.png', '*.jpg', '*.jpeg')\n",
        "    image_paths = []\n",
        "    for ext in extensions:\n",
        "        image_paths.extend(glob.glob(os.path.join(folder_path, '**/' + ext), recursive=True))\n",
        "\n",
        "    if not image_paths:\n",
        "        logging.warning(f\"No images found in {folder_path}\")\n",
        "        return []\n",
        "\n",
        "    # Filter out already processed images\n",
        "    processed = load_processed_files()\n",
        "    to_process = [img for img in image_paths if img not in processed]\n",
        "\n",
        "    if not to_process:\n",
        "        logging.info(f\"All images in {folder_path} already processed\")\n",
        "        return []\n",
        "\n",
        "    # Process in batches\n",
        "    batch_size = 500\n",
        "    all_results = []\n",
        "    for i in range(0, len(to_process), batch_size):\n",
        "        batch = to_process[i:i+batch_size]\n",
        "        logging.info(f\"Processing batch {i//batch_size + 1} in {folder_path} ({len(batch)} images)\")\n",
        "\n",
        "        with Pool(cpu_count()) as pool:\n",
        "            results = list(tqdm(pool.imap(extract_features, batch), total=len(batch)))\n",
        "\n",
        "        valid_results = [r for r in results if r is not None]\n",
        "        if valid_results:\n",
        "            all_results.extend(valid_results)\n",
        "            update_processed_log(valid_results)\n",
        "\n",
        "        # Clean up memory\n",
        "        del results\n",
        "        import gc; gc.collect()\n",
        "\n",
        "    return all_results\n",
        "\n",
        "def process_all_folders():\n",
        "    \"\"\"Process all folders and save individual + combined results\"\"\"\n",
        "    for folder in IMAGE_FOLDERS:\n",
        "        logging.info(f\"Starting processing for {folder}\")\n",
        "        results = process_folder(folder)\n",
        "\n",
        "        if results:\n",
        "            save_folder_results(folder, results)\n",
        "\n",
        "    # Combine all results after processing all folders\n",
        "    combine_all_results()\n",
        "    logging.info(\"All processing complete!\")"
      ],
      "metadata": {
        "id": "IFixbrLg00Yj"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXECUTION**"
      ],
      "metadata": {
        "id": "5Ytu_KoY1Lm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    process_all_folders()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXdPHqQy1F3A",
        "outputId": "903d0bb9-ee03-441d-c357-3d7984f63b6f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [02:03<00:00,  4.05it/s]\n",
            "100%|██████████| 500/500 [02:06<00:00,  3.95it/s]\n",
            "100%|██████████| 500/500 [01:13<00:00,  6.81it/s]\n",
            "100%|██████████| 500/500 [01:15<00:00,  6.61it/s]\n",
            "100%|██████████| 500/500 [01:11<00:00,  6.96it/s]\n",
            "100%|██████████| 500/500 [01:09<00:00,  7.16it/s]\n",
            "100%|██████████| 500/500 [01:11<00:00,  7.04it/s]\n",
            "100%|██████████| 500/500 [01:12<00:00,  6.94it/s]\n",
            "100%|██████████| 500/500 [01:11<00:00,  7.01it/s]\n",
            "100%|██████████| 500/500 [01:09<00:00,  7.15it/s]\n",
            "100%|██████████| 500/500 [01:12<00:00,  6.93it/s]\n",
            "100%|██████████| 500/500 [01:12<00:00,  6.88it/s]\n",
            "100%|██████████| 500/500 [01:14<00:00,  6.70it/s]\n",
            "100%|██████████| 500/500 [01:10<00:00,  7.13it/s]\n",
            "100%|██████████| 500/500 [01:11<00:00,  6.95it/s]\n",
            "100%|██████████| 500/500 [01:11<00:00,  6.95it/s]\n",
            "100%|██████████| 500/500 [01:11<00:00,  6.98it/s]\n",
            "100%|██████████| 500/500 [01:10<00:00,  7.14it/s]\n",
            "100%|██████████| 500/500 [01:10<00:00,  7.13it/s]\n",
            "100%|██████████| 500/500 [01:11<00:00,  7.02it/s]\n",
            "100%|██████████| 500/500 [01:14<00:00,  6.72it/s]\n",
            "100%|██████████| 500/500 [01:11<00:00,  7.01it/s]\n",
            "100%|██████████| 500/500 [01:10<00:00,  7.08it/s]\n",
            "100%|██████████| 500/500 [01:12<00:00,  6.91it/s]\n",
            "100%|██████████| 500/500 [01:13<00:00,  6.82it/s]\n",
            "100%|██████████| 500/500 [01:16<00:00,  6.52it/s]\n",
            "100%|██████████| 500/500 [01:16<00:00,  6.52it/s]\n",
            "100%|██████████| 500/500 [01:15<00:00,  6.60it/s]\n",
            "100%|██████████| 500/500 [01:14<00:00,  6.67it/s]\n",
            "100%|██████████| 500/500 [01:14<00:00,  6.68it/s]\n",
            "100%|██████████| 500/500 [01:18<00:00,  6.38it/s]\n",
            "100%|██████████| 500/500 [01:11<00:00,  7.00it/s]\n",
            "100%|██████████| 500/500 [01:11<00:00,  7.03it/s]\n",
            "100%|██████████| 500/500 [01:11<00:00,  6.98it/s]\n",
            "100%|██████████| 500/500 [01:11<00:00,  6.96it/s]\n",
            "100%|██████████| 500/500 [01:10<00:00,  7.11it/s]\n",
            "100%|██████████| 500/500 [01:12<00:00,  6.90it/s]\n",
            "100%|██████████| 500/500 [01:15<00:00,  6.64it/s]\n",
            "100%|██████████| 500/500 [01:15<00:00,  6.66it/s]\n",
            "100%|██████████| 500/500 [01:13<00:00,  6.84it/s]\n",
            "100%|██████████| 500/500 [01:15<00:00,  6.60it/s]\n",
            "100%|██████████| 500/500 [01:09<00:00,  7.16it/s]\n",
            "100%|██████████| 500/500 [01:11<00:00,  7.00it/s]\n",
            "100%|██████████| 500/500 [01:13<00:00,  6.85it/s]\n",
            "100%|██████████| 500/500 [01:14<00:00,  6.71it/s]\n",
            "100%|██████████| 500/500 [01:15<00:00,  6.65it/s]\n",
            "100%|██████████| 500/500 [01:15<00:00,  6.60it/s]\n",
            "100%|██████████| 500/500 [01:15<00:00,  6.61it/s]\n",
            "100%|██████████| 500/500 [01:15<00:00,  6.66it/s]\n",
            "100%|██████████| 500/500 [01:14<00:00,  6.71it/s]\n",
            "100%|██████████| 500/500 [01:13<00:00,  6.82it/s]\n",
            "100%|██████████| 500/500 [01:16<00:00,  6.53it/s]\n",
            "100%|██████████| 500/500 [01:22<00:00,  6.06it/s]\n",
            "100%|██████████| 500/500 [01:16<00:00,  6.53it/s]\n",
            "100%|██████████| 500/500 [01:15<00:00,  6.63it/s]\n",
            "100%|██████████| 500/500 [01:15<00:00,  6.64it/s]\n",
            "100%|██████████| 500/500 [01:17<00:00,  6.45it/s]\n",
            "100%|██████████| 500/500 [01:17<00:00,  6.46it/s]\n",
            "100%|██████████| 500/500 [01:17<00:00,  6.47it/s]\n",
            "100%|██████████| 500/500 [01:16<00:00,  6.55it/s]\n",
            "100%|██████████| 500/500 [01:21<00:00,  6.13it/s]\n",
            "100%|██████████| 500/500 [01:17<00:00,  6.46it/s]\n",
            "100%|██████████| 500/500 [01:15<00:00,  6.59it/s]\n",
            "100%|██████████| 500/500 [01:15<00:00,  6.65it/s]\n",
            "100%|██████████| 500/500 [01:16<00:00,  6.51it/s]\n",
            "100%|██████████| 500/500 [01:17<00:00,  6.49it/s]\n",
            "100%|██████████| 500/500 [01:15<00:00,  6.63it/s]\n",
            "100%|██████████| 457/457 [01:07<00:00,  6.78it/s]\n",
            "100%|██████████| 500/500 [05:57<00:00,  1.40it/s]\n",
            "100%|██████████| 500/500 [01:12<00:00,  6.94it/s]\n",
            "100%|██████████| 500/500 [01:13<00:00,  6.83it/s]\n",
            "100%|██████████| 500/500 [01:14<00:00,  6.69it/s]\n",
            "100%|██████████| 500/500 [01:15<00:00,  6.65it/s]\n",
            "100%|██████████| 500/500 [01:15<00:00,  6.62it/s]\n",
            "100%|██████████| 500/500 [01:21<00:00,  6.16it/s]\n",
            "100%|██████████| 500/500 [01:16<00:00,  6.50it/s]\n",
            "100%|██████████| 500/500 [01:15<00:00,  6.59it/s]\n",
            "100%|██████████| 500/500 [01:14<00:00,  6.69it/s]\n",
            "100%|██████████| 500/500 [01:15<00:00,  6.63it/s]\n",
            "100%|██████████| 500/500 [01:16<00:00,  6.57it/s]\n",
            "100%|██████████| 500/500 [01:15<00:00,  6.59it/s]\n",
            "100%|██████████| 500/500 [01:17<00:00,  6.46it/s]\n",
            "100%|██████████| 500/500 [01:16<00:00,  6.52it/s]\n",
            "100%|██████████| 500/500 [01:15<00:00,  6.66it/s]\n",
            "100%|██████████| 500/500 [01:15<00:00,  6.65it/s]\n",
            "100%|██████████| 500/500 [01:14<00:00,  6.69it/s]\n",
            "100%|██████████| 500/500 [01:16<00:00,  6.53it/s]\n",
            "100%|██████████| 500/500 [01:15<00:00,  6.60it/s]\n",
            "100%|██████████| 500/500 [01:16<00:00,  6.58it/s]\n",
            "100%|██████████| 277/277 [00:42<00:00,  6.45it/s]\n"
          ]
        }
      ]
    }
  ]
}