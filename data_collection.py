# -*- coding: utf-8 -*-
"""Data_collection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ItF7QnC94DoQ88uENas4aYAJHG5NT7mM

**Install Dependensies**
"""

!pip install huggingface_hub
!pip install tqdm

"""**2. IMPORTS**"""

import os
import tarfile
from concurrent.futures import ThreadPoolExecutor
import multiprocessing
from huggingface_hub import HfApi, hf_hub_download
import logging
import cv2
import numpy as np
import multiprocessing
from google.colab import drive
import logging
from tqdm import tqdm
import shutil

"""**3. Setup & Configuration**"""

# Mount Google Drive
drive.mount('/content/drive')

# Configuration
HF_TOKEN = "use your toke"
DATASET_REPO = "xingjunm/WildDeepfake"
OUTPUT_DIR = "/content/drive/MyDrive/DeepfakeImage"
CATEGORIES = ['real_train', 'real_test', 'fake_train', 'fake_test']

# Create output directories
for category in CATEGORIES:
    os.makedirs(os.path.join(OUTPUT_DIR, category), exist_ok=True)

# Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

"""**Check Existing Downloads**"""

# ===== 1. CHECK FOR EXISTING IMAGES =====
def get_existing_images():
    """Returns set of already downloaded image paths"""
    existing = set()
    for category in CATEGORIES:
        category_dir = os.path.join(OUTPUT_DIR, category)
        if os.path.exists(category_dir):
            for img in os.listdir(category_dir):
                if img.lower().endswith('.png'):
                    existing.add(f"{category}/{img}")
    return existing

existing_images = get_existing_images()
logging.info(f"Found {len(existing_images)} pre-existing images")

"""**Download Function**"""

def download_and_extract(repo_id, file_path):
    """Downloads and extracts only new images"""
    try:
        # Determine category
        category = next((cat for cat in CATEGORIES if f'/{cat}/' in file_path), None)
        if not category:
            return 0

        # Download the tar file
        local_path = hf_hub_download(
            repo_id=repo_id,
            filename=file_path,
            token=HF_TOKEN,
            repo_type="dataset",
            cache_dir="tmp_downloads"
        )

        # Process archive
        count = 0
        with tarfile.open(local_path, "r:*") as tar:
            for member in tar.getmembers():
                if member.isfile() and member.name.lower().endswith('.png'):
                    img_name = os.path.basename(member.name)
                    img_key = f"{category}/{img_name}"

                    # Skip if already exists
                    if img_key in existing_images:
                        continue

                    try:
                        # Extract and save new image
                        f = tar.extractfile(member)
                        img_data = f.read()
                        save_path = os.path.join(OUTPUT_DIR, category, img_name)

                        with open(save_path, 'wb') as img_file:
                            img_file.write(img_data)

                        count += 1
                        # Add to existing set to prevent duplicates in same run
                        existing_images.add(img_key)

                    except Exception as e:
                        logging.error(f"Error processing {member.name}: {str(e)}")

        os.remove(local_path)
        return count

    except Exception as e:
        logging.error(f"Failed to process {file_path}: {str(e)}")
        return 0

"""**3. MAIN EXECUTION**"""

def main():
    """Main workflow with duplicate prevention"""
    # Clean temporary directory
    if os.path.exists("tmp_downloads"):
        shutil.rmtree("tmp_downloads")

    # Get list of all tar files
    api = HfApi(token=HF_TOKEN)
    all_files = api.list_repo_files(repo_id=DATASET_REPO, repo_type="dataset")
    tar_files = [f for f in all_files if f.endswith('.tar.gz') and any(f'/{cat}/' in f for cat in CATEGORIES)]

    # Process files in parallel
    with ThreadPoolExecutor(max_workers=multiprocessing.cpu_count()) as executor:
        futures = []
        for file_path in tar_files:
            futures.append(executor.submit(
                download_and_extract,
                repo_id=DATASET_REPO,
                file_path=file_path
            ))

        # Progress tracking
        total_new = 0
        for i, future in enumerate(futures):
            new_count = future.result()
            total_new += new_count
            logging.info(f"File {i+1}/{len(tar_files)}: Added {new_count} new images | Total: {total_new}")

    logging.info(f"Download complete! New images: {total_new}")
    logging.info(f"Total images now in {OUTPUT_DIR}: {len(get_existing_images())}")

if __name__ == "__main__":
    main()